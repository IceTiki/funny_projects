# 英语一真题(2010~2022)单词数据分析

## 摘要

本次统计根据考纲5500单词进行统计(超纲单词未纳入统计)，统计范围是2010到2022的英语一试卷。共统计单词**50159**个。

## 单词统计

> 图——单词在<u>考试中的出现次数</u> (从高到低)。下方50个高频词出现次数19153，占考试中单词总数的38.18%
>
> ![Figure_1](英语一真题(2010~2022)单词数据分析.assets/Figure_1-16489107034673.png)
>
> ![Figure_1-16489106681192](英语一真题(2010~2022)单词数据分析.assets/Figure_1-16489106681192.png)
>
> ```python
> a = words[0:50]
> x = [i['word'] for i in a]
> y = [i['sum'] for i in a]
> plt.bar(x,y)
> y_label = ["{:d}".format(_y) for _y in y]
> for a, b, label in zip(x, y, y_label):
> plt.text(a, b, label, ha='center', va='bottom', rotation = 300)
> plt.xticks(rotation = 300)
> plt.show()
> ```

## 低频词频率

> 图——不同出现频率单词在考试中的出现次数。可以看到低频词虽然频率低，但是数量多，反而出现次数并不少。
>
> * 特低频词——出现次数少于10次——出现次数为7676——占考试单词的15.30%
> * 低频词——出现次数少于20次——出现次数为14212——占考试单词的28.33%
> * 较低频词——出现次数少于50次——出现次数为24123——占考试单词的48.09%
>
> ![Figure_1](英语一真题(2010~2022)单词数据分析.assets/Figure_1-16489110108245.png)
>
> ```python
> r = 80
> data = [0]*r
> for i in range(r):
>     for j in words:
>         if sum(j['distributing']) == i:
>             data[i] += 1
> y = [i*v for i,v in enumerate(data)]
> x = range(r)
> plt.bar(x,y)
> y_label = ["{:d}".format(_y) for _y in y]
> for a, b, label in zip(x, y, y_label):
>     plt.text(a, b, label, ha='center', va='bottom', rotation = 300)
> plt.show()
> ```

> 图——在5500词中，不同出现频率单词的个数。
>
> 可以看到零频词占比高达38.58%，出现次数不足5次的词占比高达71.85%。
>
> ![Figure_1](英语一真题(2010~2022)单词数据分析.assets/Figure_1-16489128071026.png)

## 出现稳定性

> 图：在5500词中，不同 出现分布的方差[^1] 单词的个数。
>
> 方差为0的占比39.69%，多是零频率单词。方差为11占比18.36%，多是只出现1次的单词。
>
> 参考方差，可以看到方差超过0.2时，单词在每次出现频率有明显波动。
>
> | 单词     | 出现次数 | 方差                 | 分布[2010年,2011年,....,2021年]                              |
> | -------- | -------- | -------------------- | ------------------------------------------------------------ |
> | the      | 2924     | 0.019804963311319503 | [260, 196, 249, 237, 302, 266, 219, 254, 285, 172, 238, 246] |
> | and      | 997      | 0.01821613285191583  | [70, 66, 94, 76, 104, 84, 77, 76, 97, 77, 82, 94]            |
> | research | 69       | 0.4896030245746692   | [5, 4, 6, 4, 17, 9, 4, 2, 3, 3, 9, 3]                        |
> | work     | 69       | 0.26780088216761183  | [7, 1, 9, 5, 12, 4, 8, 3, 6, 5, 7, 2]                        |
> | some     | 78       | 0.10848126232741617  | [9, 6, 7, 8, 5, 6, 4, 11, 3, 5, 8, 6]                        |
> | it       | 570      | 0.05824561403508771  | [50, 37, 51, 53, 33, 41, 77, 49, 51, 34, 54, 40]             |
>
> [^1]: 这里是方差经过特殊处理，求方差之前会先将所有数据除以其平均数，以避免出现次数多的单词方差过大
>
> ![Figure_1](英语一真题(2010~2022)单词数据分析.assets/Figure_1-16489147678868.png)
>
> ```python
> r = 120
> data = [0]*r
> x = []
> for i in range(r):
>  i2 = i/10
>  x.append(i2)
>  for j in words:
>      if j['normalVariance'] < i2+0.1 and j['normalVariance'] >= i2:
>          data[i] += 1
> y = data
> plt.bar(x,y,width = 0.07)
> y_label = ['' if _y==0 else "{:.2%}".format(_y/5500) for _y in y]
> for a, b, label in zip(x, y, y_label):
>  plt.text(a, b, label, ha='center', va='bottom', rotation = 270)
> plt.show()
> ```

> 图：词语的方差与出现次数的关系。
>
> 零词频单词方差为0，只出现一次的单词方差为11。词语出现次数越多，其方差趋向减小。
>
> ![Figure_1](英语一真题(2010~2022)单词数据分析.assets/Figure_1-16489524649001.png)
>
> ![Figure_2](英语一真题(2010~2022)单词数据分析.assets/Figure_2.png)

## 有出现的考试场数

> 图：单词在考试中出现的场数。
>
> 有2122个单词，没有在考试出场。
> 只有195个单词，每次考试都会出场，而这些单词是
>
> ```json
> ["the", "of", "to", "a", "and", "in", "that", "it", "its", "for", "for", "on", "be", "being", "as", "by", "with", "not", "have", "from", "from", "their", "theirs", "or", "but", "text", "they", "more", 
> "at", "you", "can", "this", "we", "which", "one", "your", "yours", "should", "answer", "use", "used", "useful", "will", "willing", "part", "follow", "following", "follow", "following", "what", "about", "when", "such", "there", "so", "into", "direct", "direction", "than", "change", "hi", "his", "like", "likely", "some", "them", "company", "author", "authority", "sheet", "these", "out", "research", "work", "only", "many", "help", "helpful", "how", "how", "no", "word", "even", "evening", "law", "up", "Also", "could", "each", "need", "those", "accord", "accordance", "according", "section", "world", "question", "universal", "universe", "university", "now", "look", "number", "first", "get", "mean", "meaning", "means", "then", "because", "last", "mark", "own", "increase", "require", "requirement", "us", "where", "best", "business", "busy", "effect", "effective", "live", "lively", "living", "choose", "show", "must", "learn", "learned", "learning", "differ", "difference", "different", "give", "support", "place", "well", "end", "ending", "today", "after", "often", "benefit", "instead", "experience", "experiment", "another", "care", "careful", "essay", "little", "translate", "translation", "translate", "translation", "base", "hold", "title", "consider", "name", "namely", "sign", "blank", "four", "four", "within", "list", "segment", "comment", "underline"]
> ```
>
> ![Figure_1](英语一真题(2010~2022)单词数据分析.assets/Figure_1-16502518777591.png)

> ```python
> wordAppearDistributing = [0]*13
> 
> for w in words:
>     wordAppear = len(list(filter(lambda x:x,w['distributing'])))
>     wordAppearDistributing[wordAppear] += 1
> 
> print(wordAppearDistributing)
> 
> x = range(len(wordAppearDistributing))
> y = wordAppearDistributing
> 
> y_label = ["{:d}".format(_y) for _y in y]
> for a, b, label in zip(x, y, y_label):
>     plt.text(a, b, label, ha='center', va='bottom', rotation = 300)
> 
> plt.bar(x,y)
> plt.show()
> ```

## 数据前处理

1. 首先将多个word形式真题读取，以字典写入到yaml中。

   ```yaml
   2010年考研英语一真题.docx: 绝密★启用前\n201...
   2011年考研英语一真题.docx: 绝密★启用前\n201...
   2012年考研英语一真题.docx: 绝密★启用前\n201...
   ```

2. 使用自然语言处理库，对单词进行统计(统计一个单词意味着统计其所有变化形式)

   ```python
   class NatureLanguage:
       ps = PorterStemmer()
   
       @staticmethod
       def countWord(sentence: str, word: str):
           count = 0
           wordList = nltk.word_tokenize(sentence)
           for w in wordList:
               if NatureLanguage.ps.stem(w) == NatureLanguage.ps.stem(word):
                   count += 1
           return count
   ```

3. 将数据进行处理，存入yaml中

   ```yaml
   - distributing: [260, 196, 249, 237, 302, 266, 219, 254, 285, 172, 238, 246]
     frequency: 243.66666666666666
     mean: art.这(那)个;这(那)些(指特定的人或物)
     normalVariance: 0.019804963311319503
     sum: 2924
     word: the
   - distributing: [141, 117, 138, 136, 153, 160, 102, 133, 137, 109, 131, 133]
     frequency: 132.5
     mean: prep.…的;在…之中;用…制的;关于…的
     normalVariance: 0.014415569004390644
     sum: 1590
     word: of
   ```

### 基础部分代码

```python
import yaml
import docx
from nltk.stem.porter import PorterStemmer
import nltk
import numpy
import matplotlib.pyplot as plt


def loadYml(ymlFileName='yamlfile.yml'):
    with open(ymlFileName, 'r', encoding="utf-8") as f:
        return yaml.load(f, Loader=yaml.FullLoader)


def writeYml(item, ymlFileName='yamlfile.yml'):
    with open(ymlFileName, 'w', encoding='utf-8') as f:
        yaml.dump(item, f, allow_unicode=True)


def docx2txt(path):
    file = docx.opendocx(path)
    textList = docx.getdocumenttext(file)
    text = '\n'.join(textList)
    return text


class NatureLanguage:
    ps = PorterStemmer()

    @staticmethod
    def countWord(sentence: str, word: str):
        count = 0
        wordList = nltk.word_tokenize(sentence)
        for w in wordList:
            if NatureLanguage.ps.stem(w) == NatureLanguage.ps.stem(word):
                count += 1
        return count


def normalVariance(data:list):
    '''接受一个非负数列表, 将所有数值除以平均数, 然后计算方差并返回'''
    average = numpy.mean(data)
    if average:
        normalizeData = [i/average for i in data]
        return float(numpy.var(normalizeData))
    else:
        return 0

print('--------------------loadingYamlFile--------------------')
words = loadYml('word5500-distributing.yml')
```



